{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install albumentations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import cv2\n",
    "from PIL import Image, ImageChops\n",
    "from tqdm import tqdm, trange\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "import torchvision.models as models\n",
    "from torchvision.models import VGG16_Weights\n",
    "from transformers import SegformerForSemanticSegmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# choose device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def show_images(list_of_images, titles=None):\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    count = len(list_of_images)\n",
    "    for idx in range(count):\n",
    "        subplot = plt.subplot(1, count, idx+1)\n",
    "        if titles is not None:\n",
    "          subplot.set_title(titles[idx])\n",
    "          \n",
    "        img = list_of_images[idx]\n",
    "        cmap = 'gray' if (len(img.shape) == 2 or img.shape[2] == 1) else None\n",
    "        subplot.imshow(img, cmap=cmap)\n",
    "    plt.show()  \n",
    "\n",
    "def plot_losses(losses_dict):\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    for label, losses in losses_dict.items():\n",
    "        plt.plot(losses, label=label)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Validation Losses\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "class IoULoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IoULoss, self).__init__()\n",
    "        self.eps = 1e-6\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred = y_pred.view(-1)        \n",
    "        y_true = y_true.view(-1)\n",
    "\n",
    "        intersection = (y_pred * y_true).sum()\n",
    "        union = y_pred.sum() + y_true.sum() - intersection\n",
    "        \n",
    "        iou = intersection / (union + self.eps)\n",
    "        return 1 - iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class to store hyperparameters for model training\n",
    "class Config:\n",
    "    def __init__(self, model_name):\n",
    "        if model_name == \"UNet\":\n",
    "            self.epochs = 100\n",
    "            self.lr = 1e-3\n",
    "            self.loss_fn = IoULoss()\n",
    "            self.save_path = \"checkpoints/unet_best_model.pth\"\n",
    "            self.need_training = False\n",
    "        elif model_name == \"FCN\":\n",
    "            self.epochs = 100\n",
    "            self.lr = 1e-3\n",
    "            self.loss_fn = IoULoss()\n",
    "            self.save_path = \"checkpoints/fcn_best_model.pth\"\n",
    "            self.need_training = False\n",
    "        elif model_name == \"SegFormer\":\n",
    "            self.epochs = 10\n",
    "            self.lr = 5e-5  \n",
    "            self.loss_fn = IoULoss()\n",
    "            self.save_path = \"checkpoints/seg_former_best_model.pth\"\n",
    "            self.need_training = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# dataset_path = '/kaggle/input/'\n",
    "dataset_path = './content/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Dataset Path: {dataset_path}\")\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    print(f\"\\nDirectory: {root}\")\n",
    "    print(f\"  Subdirectories: {dirs}\")\n",
    "    print(f\"  Files: {files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def count_files_in_dir(directory):\n",
    "    return len([f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_path = os.path.join(dataset_path, 'training')\n",
    "calib_path = os.path.join(training_path, 'calib')\n",
    "gt_path = os.path.join(training_path, 'gt_image_2')\n",
    "img_path = os.path.join(training_path, 'image_2')\n",
    "\n",
    "print(\"Training Data Analysis:\")\n",
    "print(f\"Calibration Files in {calib_path}: {count_files_in_dir(calib_path)}\")\n",
    "print(f\"Ground Truth Files in {gt_path}: {count_files_in_dir(gt_path)}\")\n",
    "print(f\"Image Files in {img_path}: {count_files_in_dir(img_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "testing_path = os.path.join(dataset_path, 'testing')\n",
    "print(\"Testing Data Analysis:\")\n",
    "video_files = [f for f in os.listdir(testing_path) if f.endswith('.mp4')]\n",
    "print(f\"Number of Videos: {len(video_files)}\")\n",
    "\n",
    "for video in video_files:\n",
    "    video_path = os.path.join(testing_path, video)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    print(f\"\\nVideo: {video}\")\n",
    "    print(f\"  Frame Count: {frame_count}\")\n",
    "    print(f\"  FPS: {fps}\")\n",
    "    print(f\"  Resolution: {width}x{height}\")\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KITTI Road Dataset contains images and their corresponding ground truth segmentation files. However, **the number of images does not match the number of ground truth files**. This discrepancy arises because the dataset includes **additional lane label files**, which are not relevant for our current task of road segmentation.\n",
    "\n",
    "To resolve this, we will **filter out the lane label files** and ensure a **one-to-one correspondence** between the input images and the ground truth segmentation files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def filter_and_match_files(image_dir, label_dir):\n",
    "    img_files = glob.glob(os.path.join(image_dir, \"*.png\"))\n",
    "    label_files = glob.glob(os.path.join(label_dir, \"*_road_*.png\"))\n",
    "\n",
    "    img_files.sort()\n",
    "    label_files.sort()\n",
    "\n",
    "    img_filenames = [os.path.basename(f) for f in img_files]\n",
    "    label_filenames = [os.path.basename(f).replace(\"_road_\", \"_\") for f in label_files]\n",
    "\n",
    "    matching_files = [\n",
    "        (img, lbl) for img, lbl in zip(img_filenames, label_filenames) if img == lbl\n",
    "    ]\n",
    "    \n",
    "    print(f\"Total images: {len(img_files)}, Total road labels: {len(label_files)}\")\n",
    "    print(f\"Matching files: {len(matching_files)}\")\n",
    "\n",
    "    return matching_files, img_files, label_files\n",
    "    \n",
    "def convert_to_binary_mask(mask, road_label=[255, 0, 255]):\n",
    "    road_label = np.array(road_label)\n",
    "    binary_mask = (np.all(mask == road_label, axis=2)).astype(np.uint8)\n",
    "    return binary_mask\n",
    "    \n",
    "def visualize_overlay(image_path, label_path):\n",
    "    img = Image.open(image_path)\n",
    "    label = Image.open(label_path)\n",
    "\n",
    "    img_np = np.array(img)\n",
    "    label_np = np.array(label)\n",
    "\n",
    "    mask = np.array(Image.open(label_path))\n",
    "    binary_mask = convert_to_binary_mask(mask)\n",
    "    binary_img = Image.fromarray(binary_mask * 255)\n",
    "    overlay_binary = ImageChops.add(img, binary_img.convert(\"RGB\"), scale=1.7)\n",
    "    overlay_binary_np = np.array(overlay_binary)\n",
    "    \n",
    "    show_images(\n",
    "        [img_np, label_np, binary_mask, overlay_binary_np],\n",
    "        titles=[\"Image\", \"Label\", \"Binary Mask\", \"Overlay Binary\"])\n",
    "\n",
    "def visualize_training_examples(image_path, label_path):\n",
    "    img = Image.open(image_path)\n",
    "    label = Image.open(label_path)\n",
    "\n",
    "    img_np = np.array(img)\n",
    "    label_np = np.array(label)\n",
    "\n",
    "    show_images(\n",
    "        [img_np, label_np],\n",
    "        titles=[\"Image\", \"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_img_dir = os.path.join(dataset_path, \"training/image_2\")\n",
    "train_label_dir = os.path.join(dataset_path, \"training/gt_image_2\")\n",
    "\n",
    "matching_files, img_files, label_files = filter_and_match_files(\n",
    "        train_img_dir, train_label_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for i in range(5): \n",
    "    visualize_training_examples(img_files[i], label_files[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for i in range(5,10): \n",
    "    visualize_overlay(img_files[i], label_files[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KITTI Road Dataset contains images categorized into three distinct types based on road conditions and markings:\n",
    "\n",
    "1. **`uu` (Urban Unmarked)**:\n",
    "   - Roads in urban areas **without lane markings**.\n",
    "   - Example: Regular streets in cities where lane boundaries are not explicitly marked.\n",
    "\n",
    "2. **`um` (Urban Marked)**:\n",
    "   - Urban roads with **clearly marked lanes**.\n",
    "   - Example: Streets with visible lane lines that define driving paths.\n",
    "\n",
    "3. **`umm` (Urban Multiple Marked)**:\n",
    "   - Urban roads with **multiple lanes and lane markings**.\n",
    "   - Example: Complex intersections or multi-lane roads with clear markings for different lanes.\n",
    "\n",
    "Each file in the dataset is named with a prefix (`uu`, `um`, `umm`) to indicate its type. Below are visual examples of each type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def filter_images_by_prefix(image_dir, prefixes):\n",
    "    images = {prefix: [] for prefix in prefixes}\n",
    "    for prefix in prefixes:\n",
    "        images[prefix] = glob.glob(os.path.join(image_dir, f\"{prefix}_*.png\"))\n",
    "    return images\n",
    "\n",
    "def visualize_images_by_type(image_dict, num_samples=3):\n",
    "    for prefix, files in image_dict.items():\n",
    "        selected_files = files[:num_samples]\n",
    "        print(f\"Type: {prefix}\")\n",
    "        images = [np.array(Image.open(file)) for file in selected_files]\n",
    "        titles = [f\"{prefix} example {i+1}\" for i in range(len(images))]\n",
    "        show_images(images, titles=titles)\n",
    "        \n",
    "prefixes = [\"uu\", \"um\", \"umm\"]\n",
    "images_by_type = filter_images_by_prefix(train_img_dir, prefixes)\n",
    "visualize_images_by_type(images_by_type, num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "IMG_SIZE = (256, 256)\n",
    "BATCH_SIZE = 32\n",
    "AUGMENTATION_COUNT = 10\n",
    "TRAIN_RATIO = 0.8\n",
    "VAL_RATIO = 0.1\n",
    "\n",
    "# Paths\n",
    "MASK_PATH = dataset_path + '/training/gt_image_2'\n",
    "IMG_PATH = dataset_path + '/training/image_2'\n",
    "\n",
    "\n",
    "# Utility functions\n",
    "def load_images(path, mask=False):\n",
    "    \"\"\"Load and resize images from a given directory.\"\"\"\n",
    "    images = sorted([f for f in os.listdir(path) if not mask or \"road\" in f])\n",
    "    result = []\n",
    "    for img in tqdm(images, desc=f\"Loading {'masks' if mask else 'images'}\"):\n",
    "        result.append(np.asarray(Image.open(os.path.join(path, img)).resize(IMG_SIZE)))\n",
    "    return np.array(result)\n",
    "\n",
    "def convert_masks_to_binary(masks, road_label=(255, 0, 255)):\n",
    "    \"\"\"Convert masks to binary based on the road label.\"\"\"\n",
    "    binary_masks = []\n",
    "    for mask in tqdm(masks, desc=\"Converting masks to binary\"):\n",
    "        binary_mask = np.all(mask == road_label, axis=-1).astype(np.float32)\n",
    "        binary_masks.append(np.expand_dims(binary_mask, axis=-1))\n",
    "    return np.array(binary_masks)\n",
    "\n",
    "def normalize_images(images):\n",
    "    \"\"\"Normalize images to the range [0, 1].\"\"\"\n",
    "    return images / 255.0\n",
    "\n",
    "def augment_data(images, masks, pipeline, augment_count):\n",
    "    \"\"\"Apply augmentations to images and masks.\"\"\"\n",
    "    augmented_images, augmented_masks = [], []\n",
    "    for img, mask in tqdm(zip(images, masks), desc=\"Augmenting data\", total=len(images)):\n",
    "        img = img.astype(np.float32)\n",
    "        mask = mask.astype(np.float32)\n",
    "        for _ in range(augment_count):\n",
    "            augmented = pipeline(image=img, mask=mask)\n",
    "            augmented_images.append(augmented['image'])\n",
    "            augmented_masks.append(augmented['mask'])\n",
    "    return np.array(augmented_images), np.array(augmented_masks)\n",
    "\n",
    "def split_data(data, labels, train_ratio, val_ratio):\n",
    "    \"\"\"Split data into training, validation, and test sets.\"\"\"\n",
    "    total_samples = len(data)\n",
    "    train_size = int(total_samples * train_ratio)\n",
    "    val_size = int(total_samples * val_ratio)\n",
    "    indices = np.random.permutation(total_samples)\n",
    "\n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:train_size + val_size]\n",
    "    test_indices = indices[train_size + val_size:]\n",
    "\n",
    "    return (\n",
    "        data[train_indices], labels[train_indices],\n",
    "        data[val_indices], labels[val_indices],\n",
    "        data[test_indices], labels[test_indices]\n",
    "    )\n",
    "\n",
    "# Dataset class\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, images, masks):\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.masks[idx]\n",
    "\n",
    "# Load and preprocess data\n",
    "images = load_images(IMG_PATH)\n",
    "masks = load_images(MASK_PATH, mask=True)\n",
    "masks = convert_masks_to_binary(masks)\n",
    "images = normalize_images(images)\n",
    "\n",
    "# Augmentation pipeline\n",
    "augmentation_pipeline = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate(limit=10, p=0.5, border_mode=cv2.BORDER_CONSTANT),\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, p=0.5),\n",
    "    A.RGBShift(r_shift_limit=1, g_shift_limit=1, b_shift_limit=1, p=0.5),\n",
    "])\n",
    "\n",
    "# Apply augmentations\n",
    "augmented_images, augmented_masks = augment_data(images, masks, augmentation_pipeline, AUGMENTATION_COUNT)\n",
    "\n",
    "# Combine original and augmented data\n",
    "images = np.concatenate((images, augmented_images), axis=0)\n",
    "masks = np.concatenate((masks, augmented_masks), axis=0)\n",
    "\n",
    "# Split data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = split_data(images, masks, TRAIN_RATIO, VAL_RATIO)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = ImageDataset(X_train.transpose((0, 3, 1, 2)), y_train.transpose((0, 3, 1, 2)))\n",
    "val_dataset = ImageDataset(X_val.transpose((0, 3, 1, 2)), y_val.transpose((0, 3, 1, 2)))\n",
    "test_dataset = ImageDataset(X_test.transpose((0, 3, 1, 2)), y_test.transpose((0, 3, 1, 2)))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Print dataset sizes\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
    "print(f\"Number of test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to display images and their corresponding masks\n",
    "def show_images_from_loader(data_loader, num_images=5):\n",
    "    images_shown = 0\n",
    "    for images, masks in data_loader:\n",
    "        # Detach from torch tensors and convert to numpy\n",
    "        images = images.numpy().transpose(0, 2, 3, 1)  # Change to HWC for visualization\n",
    "        masks = masks.numpy()\n",
    "        \n",
    "        for i in range(len(images)):\n",
    "            if images_shown >= num_images:\n",
    "                return\n",
    "            plt.figure(figsize=(8, 4))\n",
    "\n",
    "            # Display image\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.imshow(images[i])\n",
    "            plt.title(\"Image\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            # Display corresponding mask\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.imshow(masks[i][0], cmap='gray')  # Display first channel of mask\n",
    "            plt.title(\"Mask\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            plt.show()\n",
    "            images_shown += 1\n",
    "\n",
    "# Display a few images and masks from train_loader\n",
    "show_images_from_loader(train_loader, num_images=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model, train_loader, val_loader, loss_fn, optimizer, num_epochs, device, save_path, patience=5\n",
    "):\n",
    "    best_val_loss = float(\"inf\")\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "\n",
    "    epoch_tqdm = trange(num_epochs, desc=\"Epochs\", position=0, leave=True)\n",
    "\n",
    "    # Early stopping variables\n",
    "    no_improvement_count = 0\n",
    "\n",
    "    for epoch in epoch_tqdm:\n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_train_losses = []\n",
    "        train_loader_tqdm = tqdm(\n",
    "            train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\", position=1, leave=False\n",
    "        )\n",
    "\n",
    "        for train_input, train_mask in train_loader_tqdm:\n",
    "            train_input = train_input.to(device).float()\n",
    "            train_mask = train_mask.to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(train_input)\n",
    "            loss = loss_fn(outputs, train_mask)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_train_losses.append(loss.item())\n",
    "            train_loader_tqdm.set_postfix({\"Batch Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "        avg_train_loss = np.mean(epoch_train_losses)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        epoch_val_losses = []\n",
    "        val_loader_tqdm = tqdm(\n",
    "            val_loader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\", position=2, leave=False\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_input, val_mask in val_loader_tqdm:\n",
    "                val_input = val_input.to(device).float()\n",
    "                val_mask = val_mask.to(device).float()\n",
    "\n",
    "                outputs = model(val_input)\n",
    "                loss = loss_fn(outputs, val_mask)\n",
    "\n",
    "                epoch_val_losses.append(loss.item())\n",
    "                val_loader_tqdm.set_postfix({\"Batch Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "        avg_val_loss = np.mean(epoch_val_losses)\n",
    "        validation_losses.append(avg_val_loss)\n",
    "\n",
    "        epoch_tqdm.set_postfix(\n",
    "            {\"Train Loss\": avg_train_loss, \"Validation Loss\": avg_val_loss}\n",
    "        )\n",
    "\n",
    "        # Check for improvement\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            no_improvement_count = 0\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "\n",
    "        # Early stopping condition\n",
    "        if no_improvement_count >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "            break\n",
    "\n",
    "    return train_losses, validation_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv,self).__init__() \n",
    "        self.dconv=nn.Sequential(nn.Conv2d(in_channels,out_channels,kernel_size=3,stride=1,padding=1,bias=False),\n",
    "                                 nn.BatchNorm2d(out_channels),\n",
    "                                 nn.ReLU(inplace=True),\n",
    "                                 nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=1,padding=1,bias=False),\n",
    "                                 nn.BatchNorm2d(out_channels),\n",
    "                                 nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.dconv(x)\n",
    "\n",
    "    \n",
    "    \n",
    "class UNET(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\n",
    "        super(UNET,self).__init__()\n",
    "        \n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Down part of UNET\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Up part of UNET\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2)) \n",
    "            self.ups.append(DoubleConv(feature*2, feature))\n",
    "\n",
    "            \n",
    "        self.left_over = DoubleConv(features[-1], features[-1]*2) \n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1) \n",
    "\n",
    "    def forward(self,x):\n",
    "        skip_connections = []\n",
    "        \n",
    "        for layer in self.downs:\n",
    "            x=layer(x)\n",
    "            skip_connections.append(x)\n",
    "            x=self.pool(x)\n",
    "        \n",
    "        x=self.left_over(x)\n",
    "        skip_connections=skip_connections[::-1]\n",
    "        for idx in range(0,len(self.ups), 2): \n",
    "            \n",
    "            sc=skip_connections[idx//2]\n",
    "            x=self.ups[idx](x) \n",
    "            \n",
    "            if x.shape != sc.shape:\n",
    "                x = TF.resize(x, size=sc.shape[2:])\n",
    "\n",
    "            concat_skip = torch.cat((sc, x), dim=1) \n",
    "            x = self.ups[idx+1](concat_skip) \n",
    "\n",
    "\n",
    "        return torch.sigmoid(self.final_conv(x))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# define model and config for training\n",
    "config = Config('UNet')\n",
    "unet_model = UNET().to(device)\n",
    "optimizer = optim.Adam(unet_model.parameters(), lr=config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T15:47:41.481053Z",
     "iopub.status.busy": "2024-11-25T15:47:41.480680Z",
     "iopub.status.idle": "2024-11-25T15:47:41.492788Z",
     "shell.execute_reply": "2024-11-25T15:47:41.491726Z",
     "shell.execute_reply.started": "2024-11-25T15:47:41.481008Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if config.need_training:\n",
    "    # train the model and plot losses\n",
    "    train_losses, val_losses = train_model(\n",
    "        model=unet_model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        loss_fn=config.loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=config.epochs,\n",
    "        device=device,      \n",
    "        save_path=config.save_path\n",
    "    )\n",
    "    \n",
    "    plot_losses({\"Train Loss\": train_losses, \"Validation Loss\": val_losses})\n",
    "else:\n",
    "    unet_model.load_state_dict(torch.load(config.save_path, map_location=torch.device('cpu')))\n",
    "    unet_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(data_loader, desc=\"Evaluating Model\"):\n",
    "            images = images.to(device).float()\n",
    "            masks = masks.to(device).float()\n",
    "\n",
    "            # Predict\n",
    "            predictions = model(images)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(predictions, masks)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    print(f\"Average IoU Loss on Test Set: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def visualize_inference(model, test_loader, device, num_samples=3):\n",
    "    \"\"\"\n",
    "    Visualize inference results on a few images from the test dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    samples_shown = 0\n",
    "\n",
    "    for images, masks in test_loader:\n",
    "        images = images.to(device).float()\n",
    "        masks = masks.to(device).float()\n",
    "\n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            predictions = model(images)\n",
    "            predictions = (predictions > 0.5).float()\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            if samples_shown >= num_samples:\n",
    "                return\n",
    "\n",
    "            # Prepare images for display\n",
    "            image = images[i].cpu().numpy().transpose(1, 2, 0)  \n",
    "            original_mask = masks[i].cpu().numpy().squeeze() \n",
    "            predicted_mask = predictions[i].cpu().numpy().squeeze()  \n",
    "\n",
    "            # Display results\n",
    "            show_images(\n",
    "                [image, original_mask, predicted_mask],\n",
    "                titles=[\"Image\", \"Original Mask\", \"Predicted Mask\"],\n",
    "            )\n",
    "            samples_shown += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "avg_test_loss_unet = evaluate_model(unet_model, test_loader, config.loss_fn, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "visualize_inference(unet_model, test_loader, device, num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    def __init__(self, num_of_classes=1, height=512, width=512):\n",
    "        super(FCN, self).__init__()\n",
    "        # Encoder (VGG16 features)\n",
    "        self.vgg16_model = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features\n",
    "        for parameter in self.vgg16_model.parameters():\n",
    "            parameter.requires_grad = False\n",
    "\n",
    "        # Decoder\n",
    "        self.num_of_classes = num_of_classes\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "        self.skip_layer_4 = nn.Conv2d(in_channels=512, out_channels=256,\n",
    "                                      kernel_size=(1, 1), stride=1, padding=0)\n",
    "        self.skip_layer_3 = nn.Conv2d(in_channels=256, out_channels=128,\n",
    "                                      kernel_size=(1, 1), stride=1, padding=0)\n",
    "\n",
    "        self.upsampling_1 = nn.ConvTranspose2d(in_channels=512, out_channels=256,\n",
    "                                               kernel_size=2, stride=2, padding=0)\n",
    "        self.upsampling_2 = nn.ConvTranspose2d(in_channels=256, out_channels=128,\n",
    "                                               kernel_size=2, stride=2, padding=0)\n",
    "        self.upsampling_3 = nn.ConvTranspose2d(in_channels=128, out_channels=64,\n",
    "                                               kernel_size=2, stride=2, padding=0)\n",
    "        self.upsampling_4 = nn.ConvTranspose2d(in_channels=64, out_channels=32,\n",
    "                                               kernel_size=2, stride=2, padding=0)\n",
    "        self.upsampling_5 = nn.ConvTranspose2d(in_channels=32, out_channels=16,\n",
    "                                               kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        self.max_pooling = nn.AdaptiveMaxPool3d(output_size=(1, self.height, self.width))\n",
    "\n",
    "        self.batch_norm_1 = nn.BatchNorm2d(256)\n",
    "        self.batch_norm_2 = nn.BatchNorm2d(128)\n",
    "        self.batch_norm_3 = nn.BatchNorm2d(64)\n",
    "        self.batch_norm_4 = nn.BatchNorm2d(32)\n",
    "        self.batch_norm_5 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(16, num_of_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, image):\n",
    "        # Encoder part\n",
    "        features = []\n",
    "        num_of_layers = len(self.vgg16_model._modules)\n",
    "        x = image\n",
    "        for layer_idx in range(num_of_layers):\n",
    "            key_in_model = str(layer_idx)\n",
    "            cur_layer = self.vgg16_model._modules[key_in_model]\n",
    "            x = cur_layer(x)\n",
    "            if layer_idx == 16 or layer_idx == 23 or layer_idx == 30:\n",
    "                features.append(x)\n",
    "        \n",
    "        features_3, features_4, features_7 = features[0], features[1], features[2]\n",
    "\n",
    "        # Decoder part\n",
    "        vgg_layer_4_raw = self.skip_layer_4(features_4)\n",
    "        vgg_layer_3_raw = self.skip_layer_3(features_3)\n",
    "\n",
    "        x = F.relu(self.upsampling_1(features_7))\n",
    "        x = x.add(vgg_layer_4_raw)\n",
    "        x = self.batch_norm_1(x)\n",
    "\n",
    "        x = F.relu(self.upsampling_2(x))\n",
    "        x = x.add(vgg_layer_3_raw)\n",
    "        x = self.batch_norm_2(x)\n",
    "\n",
    "        x = F.relu(self.upsampling_3(x))\n",
    "        x = self.batch_norm_3(x)\n",
    "\n",
    "        x = F.relu(self.upsampling_4(x))\n",
    "        x = self.batch_norm_4(x)\n",
    "\n",
    "        x = F.relu(self.upsampling_5(x))\n",
    "        x = self.batch_norm_5(x)\n",
    "        \n",
    "        return torch.sigmoid(self.final_conv(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# define model and config for training\n",
    "config = Config('FCN')\n",
    "fcn_model = FCN().to(device)\n",
    "optimizer = optim.Adam(fcn_model.parameters(), lr=config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if config.need_training:\n",
    "    # train the model and plot losses\n",
    "    train_losses, val_losses = train_model(\n",
    "        model=fcn_model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        loss_fn=config.loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=config.epochs,\n",
    "        device=device,\n",
    "        save_path=config.save_path\n",
    "    )\n",
    "\n",
    "    plot_losses({\"Train Loss\": train_losses, \"Validation Loss\": val_losses})\n",
    "else:\n",
    "    fcn_model.load_state_dict(torch.load(config.save_path, map_location=torch.device('cpu')))\n",
    "    fcn_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_test_loss_fcn = evaluate_model(fcn_model, test_loader, config.loss_fn, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_inference(fcn_model, test_loader, device, num_samples=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State-Of-The-Art model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SegFormer(nn.Module):\n",
    "    def __init__(self, num_classes=1, image_size=(256, 256)):\n",
    "        super(SegFormer, self).__init__()\n",
    "        # Load pre-trained SegFormer model with a lightweight decoder head\n",
    "        self.segformer = SegformerForSemanticSegmentation.from_pretrained(\n",
    "            \"nvidia/segformer-b0-finetuned-ade-512-512\",\n",
    "            num_labels=num_classes,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        # Adjust the final classification head for the desired output size\n",
    "        self.segformer.config.hidden_size = 256\n",
    "        self.image_size = image_size\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.segformer(pixel_values=x)\n",
    "        logits = outputs.logits  # Segmentation logits\n",
    "        logits = F.interpolate(logits, size=self.image_size, mode=\"bilinear\", align_corners=False)\n",
    "        return torch.sigmoid(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# define model and config for training\n",
    "config = Config('SegFormer')\n",
    "seg_former_model = SegFormer().to(device)\n",
    "optimizer = optim.Adam(seg_former_model.parameters(), lr=config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if config.need_training:\n",
    "    train_losses, val_losses = train_model(\n",
    "        model=seg_former_model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        loss_fn=config.loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=config.epochs,\n",
    "        device=device,\n",
    "        save_path=config.save_path\n",
    "    )\n",
    "\n",
    "    plot_losses({\"Train Loss\": train_losses, \"Validation Loss\": val_losses})\n",
    "else:\n",
    "    seg_former_model.load_state_dict(torch.load(config.save_path, map_location=torch.device('cpu')))\n",
    "    seg_former_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluate SegFormer Model\n",
    "avg_test_loss_segformer = evaluate_model(seg_former_model, test_loader, config.loss_fn, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization for SegFormer\n",
    "visualize_inference(seg_former_model, test_loader, device, num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "- Lab 2 from F24 - Computer Vision course\n",
    "- https://www.kaggle.com/code/hossamemamo/kitti-road-segmentation-pytorch-unet-from-scratch\n",
    "- https://www.youtube.com/watch?v=cPOtULagNnI\n",
    "- https://www.kaggle.com/datasets/sakshaymahna/kittiroadsegmentation \n",
    "- https://www.kaggle.com/code/sakshaymahna/fully-convolutional-network/input\n",
    "- https://www.kaggle.com/code/satyaprakashshukl/road-segmentation-using-unet-model\n",
    "- https://www.kaggle.com/code/hossamemamo/kitti-road-segmentation-pytorch-unet-from-scratch\n",
    "- https://arxiv.org/abs/2105.15203"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1668350,
     "sourceId": 2736560,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "sourceId": 178295,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
